{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning?"
      ],
      "metadata": {
        "id": "ms-xMhBdclhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Reinforcement learning is a machine learning training method based on rewarding desired behaviors and/or punishing undesired ones. In general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.\n",
        "\n",
        "* Reinforcement Learning is a type of learning methodology in ML along with supervised and unsupervised learning. But, when we compare these three, reinforcement learning is a bit different than the other two. Here, we take the concept of giving rewards for every positive result and make that the base of our algorithm.\n",
        "\n",
        "* For an easier explanation, let’s take the example of a dog.\n",
        "\n",
        "* We can train our dog to perform certain actions, of course, it won’t be an easy task. You would order the dog to do certain actions and for every proper execution, you would give a biscuit as a reward. The dog will remember that if it does a certain action, it would get biscuits. This way it will follow the instructions properly next time.\n",
        "\n",
        "* We can take another example, in this case, a human child.\n",
        "\n",
        "* Kids often make mistakes. Adults try to make sure they learn from it and try not to repeat it again. In this case, we can take the concept of feedbacks. If the parents are strict, they will scold the children for any mistakes. This is a negative type of feedback. The child will remember it as if it does a certain wrong action, the parents will scold the kid.\n",
        "\n",
        "* Then there is positive feedback, where the parent might praise them for doing something right. This type of learning is called enforced learning.\n",
        "\n",
        "* Here, we enforce or try to force a correct action in a certain way.\n",
        "\n",
        "* So, in short, reinforcement learning is the type of learning methodology where we give rewards of feedback to the algorithm to learn from and improve future results.\n",
        "\n",
        "* This type of learning is on the many research fields on a global scale, as it is a big help to technologies like AI."
      ],
      "metadata": {
        "id": "9UU1-vrVcln_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advantages of Reinforcement Learning\n"
      ],
      "metadata": {
        "id": "J-GDdjKGcltg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It can solve higher-order and complex problems. Also, the solutions obtained will be very accurate.\n",
        "\n",
        "* The reason for its perfection is that it is very similar to the human learning technique.\n",
        "\n",
        "* This model will undergo a rigorous training process that can take time. This can help to correct any errors.\n",
        "\n",
        "* Due to it’s learning ability, it can be used with neural networks. This can be termed as deep reinforcement learning.\n",
        "\n",
        "* Since the model learns constantly, a mistake made earlier would be unlikely to occur in the future.\n",
        "\n",
        "* Various problem-solving models are possible to build using reinforcement learning.\n",
        "\n",
        "* When it comes to creating simulators, object detection in automatic cars, robots, etc., reinforcement learning plays a great role in the models.\n",
        "\n",
        "* The best part is that even when there is no training data, it will learn through the experience it has from processing the training data.\n",
        "\n",
        "* For various problems, which might seem complex to us, it provides the perfect models to tackle them."
      ],
      "metadata": {
        "id": "YpY_jWDAcl1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disadvantages of Reinforcement Learning"
      ],
      "metadata": {
        "id": "3iunYgPrcl7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The usage of reinforcement learning models for solving simpler problems won’t be correct. The reason being, the models generally tackle complex problems.\n",
        "\n",
        "* We will be wasting unnecessary processing power and space by using it for simpler problems.\n",
        "\n",
        "* We need lots of data to feed the model for computation. Reinforcement Learning models require a lot of training data to develop accurate results.\n",
        "\n",
        "* This consumes time and lots of computational power.\n",
        "\n",
        "* When it comes to building models on real-world examples, the maintenance cost is very high.\n",
        "\n",
        "* Like for building driverless vehicles, robots, we would require a lot of maintenance for both hardware and software.\n",
        "\n",
        "* Excessive training can lead to overloading of the states of the model. This will result in the model for getting the result.\n",
        "\n",
        "* This may happen if too much memory space goes out in processing the training data."
      ],
      "metadata": {
        "id": "GtCRp-lscmYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications of Reinforcement Learning"
      ],
      "metadata": {
        "id": "xhtUqFGf8sbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/08/Application-of-Reinforcement-Learning-TV.jpg)"
      ],
      "metadata": {
        "id": "CKW9G7tm8sUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Reinforcement learning is a vast learning methodology and its concepts can be used with other advanced technologies as well.\n",
        "\n",
        "* Here, we have certain applications, which have an impact in the real world:"
      ],
      "metadata": {
        "id": "0cwcp1VD8sKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reinforcement Learning in Business, Marketing, and Advertising\n",
        "\n",
        "* In money-oriented fields, technology can play a crucial role. Like, here RL models of companies can analyze customer preferences and help in the better advertisement of the products.\n",
        "\n",
        "* We know that business requires proper strategizing. The steps need careful planning for a product or the company to gain profit.\n",
        "\n",
        "* RL here helps to devise proper strategies by analyzing various possibilities and by that; it tries to improve the profit margin in each result. Various multinational companies use these models. Also, the cost of these models is high.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJrXFUhDcmcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reinforcement Learning in Gaming\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N2Pa01fe9lpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One of the prime usages of RL is in gaming. We have various high-end algorithms already existing in the market.\n",
        "\n",
        "* Gaming is a booming industry and is gradually advancing with technology. The games are now becoming more realistic and have many more details for them.\n",
        "\n",
        "* We have environments like PSXLE or PlayStation Reinforcement Learning Environment that focus on providing better gaming environments by modifying the emulators.\n",
        "\n",
        "* We have Deep learning algorithms like AlphaGo, AlphaZero that are gaming algorithms for games like chess, shogi and go.\n",
        "\n",
        "* With these platforms and algorithms, gaming is now more advanced and is helping in creating games, which have countless possibilities.\n",
        "\n",
        "* These can also be helpful in making story-mode games of PlayStation."
      ],
      "metadata": {
        "id": "ZU9Q0QV89lfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reinforcement Learning in Recommendation systems"
      ],
      "metadata": {
        "id": "bUsNOgdT9lVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* RL is now a big help in recommendation systems like news, music apps, and web-series apps like Netflix, etc. These apps work as per customer preferences.\n",
        "\n",
        "* In the case of web-series apps like Netflix, the variety of shows that we watch become a list of preferences for the algorithm.\n",
        "\n",
        "* Companies like these have sophisticated recommendation systems. They consider many things like user preference, trending shows, related genres, etc. Then according to these preferences, the model will show you the latest trending shows.\n",
        "\n",
        "* These models are very much cloud-based, so as users, we will use these models in our daily lives through information and entertainment platforms."
      ],
      "metadata": {
        "id": "Tj-Ih2499lHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning in Science"
      ],
      "metadata": {
        "id": "UtdJh7I39k9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* AI and ML technologies nowadays have become an important part of the research. There are various fields in science where reinforcement learning can come in handy.\n",
        "\n",
        "* The most talked-about is in atomic science. Both the physics behind atoms and their chemical properties are researched.\n",
        "\n",
        "* Reinforcement learning helps to understand chemical reactions. We can try to have cleaner reactions that yield better products. There can be various combinations of reactions for any molecule or atom. We can understand their bonding patterns with machine learning.\n",
        "\n",
        "* In most of these cases, for having better quality results, we would require deep reinforcement learning. For that, we can use some deep learning algorithms like LSTM."
      ],
      "metadata": {
        "id": "Qv36CiIX9kxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of Reinforcement Learning"
      ],
      "metadata": {
        "id": "UWnW74X09klc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positive Reinforcement Learning"
      ],
      "metadata": {
        "id": "GfWbvNd-A-_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In this type of RL, the algorithm receives a type of reward for a certain result. In other words, here we try to add a reward for every good result in order to increase the likelihood of a good result.\n",
        "\n",
        "* We can understand this easily with the help of a good example.\n",
        "\n",
        "* In order to make a child do a certain task like cleaning their rooms or study hard to get marks, some parents often promise them a reward at the end of the task.\n",
        "\n",
        "* Like, the parents promise to give the child something that he or she loves like chocolate. This rather has a good impact as it automatically makes the child work as they think of the reward. In this learning, we are adding a good reward to increase the likelihood of task completion.\n",
        "\n",
        "* This can have good impacts like improvement in performance, sustaining the change for a longer duration, etc., but its negative side could be that too much of RL could cause overloading of states that could impact the results."
      ],
      "metadata": {
        "id": "tNZxYavDA-iR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negative Reinforcement Learning"
      ],
      "metadata": {
        "id": "Sy0wcsYIA-ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This RL Type is a bit different from positive RL. Here, we try to remove something negative in order to improve performance.\n",
        "\n",
        "* We can take the same child-parent example here as well. Some parents punish kids for not cleaning their rooms.\n",
        "\n",
        "* The punishment can be no video games for one week or sometimes a month. To avoid the punishment the kids often work harder or complete the job assigned to them.\n",
        "\n",
        "* We can also take the example of getting late for the office. People often sleep late and get up late. To avoid being late at the office, they try to change their sleep habits.\n",
        "\n",
        "* From these examples, we understand that the algorithm in this case will receive negative feedback. Hence, it would avoid the process that resulted in negative feedback. This also has it’s good impacts like, the behavior toward performing the task would increase. It would force you to provide better results.\n",
        "\n",
        "* The negative impact is that it would only force you to meet the minimum necessary requirement to complete the job."
      ],
      "metadata": {
        "id": "R1q0KCebA-Ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithms of Reinforcement Learning"
      ],
      "metadata": {
        "id": "-betJLNwA-Po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There are several algorithms for reinforcement learning. We will look at the ones that we really need to know for the start.\n",
        "\n",
        "* We have certain categories in these algorithms. These are model-based and model-free algorithms.\n",
        "\n",
        "* We further classify them as on-policy or off-policy.\n",
        "\n",
        "* So, in model-based algorithms, we should have a model that would learn from current actions and from state transitions. After some steps, it would not be feasible, as it would have to store all the state and action data in the memory.\n",
        "\n",
        "* Whereas, in model-free algorithms, you do not have to worry about a model that consumes much space. This algorithm works on a trial and error basis, so you don’t need to store the states and actions.\n",
        "\n",
        "* Now, before we understand on and off policies, we need to understand a few mathematical terms."
      ],
      "metadata": {
        "id": "I9qQJVOoA-Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‘s’ is the state, ‘a’ is action, ‘π’ is the probability."
      ],
      "metadata": {
        "id": "FNLuk2wOA-FV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* There is a Q(s,a) function. This function is for predicting and giving future rewards, it does so by learning from the states and actions and giving the next values.\n",
        "\n",
        "* Whereas, ‘π’ here is for the probability to find maximum reward. So, based on this, a policy is an action ‘a’ in state ‘s’.\n",
        "\n",
        "* So, on-policy learning involves Q(s,a) learning from current state and actions, whereas, off-policy involves Q(s,a) learning from random states and actions.\n",
        "\n",
        "* Markov decision process is the root concept of this process. For understanding the use of the Markov process, you should have a good understanding of mathematics. Also, the concept is too vast to cover as it would require a separate article that would cover the whole mathematics and concept of the Markov process.\n",
        "\n",
        "* But for understanding it in this article, we will have a detailed but brief overview. The conclusions that we drew above were from a very famous formula known as the Bellman’s equation.\n",
        "\n",
        "* Also, the formula has a lot of concepts from automatas, like states and actions.\n",
        "\n",
        "* The Markov process also states that the current is very important, as it will help in determining future states. The past information is not necessary when you have a current state that depicts the same thing.\n",
        "\n",
        "* If the conditional probability of future states depend on a current state and not on the entire process before the current state, then that process has Markov property."
      ],
      "metadata": {
        "id": "0CsuRHwWA93z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fu_A2gWq9kXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning"
      ],
      "metadata": {
        "id": "C3UnSH74LxNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Q-learning is an off-policy, model-free RL algorithm. It is off-policy because the algorithm learns from random actions.\n",
        "\n",
        "* For additional information, Q here is Quality, which means the quality of the action that maximizes the reward that the algorithm gets.\n",
        "\n",
        "* When we code using this algorithm, we construct a reward matrix that stores reward at specific moves. Like if the reward is 100, then it will be stored in the matrix at the position where it got 100.\n",
        "\n",
        "* We have a program in this article based on Q-learning algorithm. There we also have added concepts like learning rate (gamma). We also have two value updating methods for Q-learning. They are Policy Iteration and Value Iteration.\n",
        "\n",
        "* Policy iteration handles policy improvement and evaluation, policy improvement is responsible for updating the policy with an action that helps in maximizing the value function. It evaluation predicts value function from the last policy improvement. Value iteration just handles the updating of values.\n",
        "\n",
        "* So, it only updates the value of the Value function. Also, in mathematical terms, we represent Q-learning as:\n",
        "\n",
        "\n",
        "```Q(s,a) = (1-α).Q(s,a) + α.(R + γ.max(Q(S2 ,a)).```\n",
        "\n",
        "\n",
        "\n",
        "* Here alpha is the learning rate. Gamma is the discount factor. R is a reward. S2 is the next state.\n",
        "\n",
        "* Q(S2 , a) is the future value. The big expression inside the bracket is the learned value."
      ],
      "metadata": {
        "id": "zmADlPFeLxFs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARSA"
      ],
      "metadata": {
        "id": "yRidi6VLLw72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The state-Action-Reward-State-Action algorithm has various similarities with the Q-learning approach. But the difference is that it is an on-policy method, unlike Q-learning, which is an off-policy method.\n",
        "\n",
        "* Q-learning learns through a greedy policy (that means learns from random actions). Whereas SARSA is on-policy, therefore it does not follow the greedy approach and it learns from the current state and actions."
      ],
      "metadata": {
        "id": "Hf31iZTHLw22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-network"
      ],
      "metadata": {
        "id": "OHRbJXu8LwsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This uses a neural network instead of the two-dimensional array. The reason is, Q-learning agents and methods can’t estimate and update values for the states that they do not know about.\n",
        "\n",
        "* In other words, if there is a completely new and unknown state, normal Q-learning won’t be able to estimate the value. Also, Q-learning follows a dynamic programming approach using the 2-D arrays.\n",
        "\n",
        "* So, in DQN we are replacing the array with neural networks, for better calculation of values."
      ],
      "metadata": {
        "id": "lr53Ac3HLwnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QkyLJgDzLwft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v0oukJ62Lwa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6d9eWrNXLwV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "terYnTv2LwQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FtSjtQJPLwLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EarHSuLiLwD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IC4EqNjMLv-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ff_b-dT6Lv3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UVYtNxDiLvwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AGTM84ZTLvm_"
      }
    }
  ]
}